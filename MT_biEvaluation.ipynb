{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsWMtWQHRSv2XFPKQIaGR5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/carinnech/pydata_bcn_NetworkX/blob/master/MT_biEvaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install py-babelnet -q\n",
        "!pip install nltk -q\n",
        "!pip install datasets -q\n",
        "!pip install translate -q\n",
        "!pip install tensorflow tensorflow_hub tensorflow_text -q\n",
        "!pip install sentencepiece -q\n",
        "!pip install transformers -q\n",
        "!pip install sacremoses -q\n",
        "!pip install bidi -q\n",
        "!pip install deep_translator -q\n",
        "!pip install sentence_transformers -q\n",
        "!pip install unidecode -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SETIgwieuV4u",
        "outputId": "82acb397-132f-427b-e373-c83ec4f04296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement bidi (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for bidi\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "z3A9j1yrt9a4",
        "outputId": "35b68455-27b2-41cc-c87f-0989a1ed3bcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 10000x10000 with 0 Axes>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 10000x10000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Imports\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle \n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text\n",
        "import torch\n",
        "from google.colab import drive\n",
        "import random\n",
        "\n",
        "# BabelNet\n",
        "import py_babelnet as pb\n",
        "from py_babelnet.calls import BabelnetAPI\n",
        "\n",
        "# Corpus imports\n",
        "from nltk.corpus import wordnet as wn\n",
        "from datasets import load_dataset_builder, get_dataset_config_names, load_dataset\n",
        "\n",
        "# Model imports \n",
        "from transformers import MarianTokenizer, MarianMTModel, MBartForConditionalGeneration, MBart50TokenizerFast, GenerationConfig\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Access to drive\n",
        "drive.mount('/content/gdrive')\n",
        "plt.figure(figsize=(20, 20), dpi=500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "DvsTBfWGuf5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Babelnet Class\n",
        "class BabelNet:\n",
        "\n",
        "  def __init__(self, searchLang, targetLang):\n",
        "    # Load Babalnet API\n",
        "    self.bn = BabelnetAPI('57865ce0-e623-49b7-9b5a-8dbd5596095a')\n",
        "    self.searchLang = searchLang\n",
        "    self.targetLang = targetLang\n",
        "    self.reqNum = 0\n",
        "\n",
        "    # Load Cache dictionary\n",
        "    self.cache = self.__load_cache()\n",
        "\n",
        "  # ------------------------------------ Cache Methods ------------------------------------ #\n",
        "\n",
        "  # Load cache from babel.pkl file in drive\n",
        "  def __load_cache(self) -> dict:\n",
        "    cache = dict()\n",
        "\n",
        "    try:\n",
        "      file = open(\"/content/gdrive/MyDrive/Carinne-Thesis/babel.pkl\", \"rb\")\n",
        "      cache = pickle.load(file)\n",
        "      file.close()\n",
        "    except:\n",
        "      print(\"No file babel.pkl. Created file\")\n",
        "      self.__save_cache(cache)\n",
        "\n",
        "    return cache\n",
        "  \n",
        "  # Save updated cache to babel.pkl file in drive\n",
        "  def __save_cache(self, cache: dict):\n",
        "    file = open(\"/content/gdrive/MyDrive/Carinne-Thesis/babel.pkl\", \"wb\")\n",
        "    pickle.dump(cache, file)\n",
        "    file.close()\n",
        "\n",
        "  # Finished with babelnet class, save cache to file\n",
        "  def save(self):\n",
        "    print(\"Number of calls: \", self.reqNum)\n",
        "    self.__save_cache(self.cache)\n",
        "\n",
        "  # ---------------------------------------- Word Senses Methods ---------------------------------------- #\n",
        "\n",
        "  # Get senses for word\n",
        "  def get_senses_of_word(self, word):\n",
        "    if (__name__, word) in self.cache:\n",
        "      senses = self.cache[(__name__, word)]\n",
        "    else:\n",
        "      senses = self.bn.get_senses(lemma = word, searchLang = self.searchLang, targetLang = self.targetLang)\n",
        "      \n",
        "      # Add number of calls and add to cache dictionary\n",
        "      self.reqNum += 1\n",
        "      self.cache[(__name__, word)] = senses\n",
        "\n",
        "    return [sense for sense in senses]\n",
        "\n",
        "  # Get senses for word excluding position of word\n",
        "  def get_senses_of_word_without_pos(self, word, pos):\n",
        "    senses = self.get_senses_of_word(word)\n",
        "    return [sense for sense in senses if sense.properties.pos != pos]\n",
        "\n",
        "  # Get number of senses for word\n",
        "  def get_senses_num(self, word):\n",
        "    return len(self.get_senses_of_word(word))\n",
        "\n",
        "  # ---------------------------------------- Sysnets Methods ---------------------------------------- #\n",
        "\n",
        "  # Get synsets ids of word\n",
        "  def get_synset_ids_word(self, word):\n",
        "    if (__name__, word) in self.cache:\n",
        "      synsets = self.cache[(__name__, word)]\n",
        "    else:\n",
        "      synsets = self.bn.get_synset_ids(lemma = word, searchLang = self.searchLang, targetLang = self.targetLang)\n",
        "      \n",
        "      # Add number of calls and add to cache dictionary\n",
        "      self.reqNum += 1\n",
        "      self.cache[(__name__, word)] = synsets\n",
        "\n",
        "    return synsets\n",
        "\n",
        "  # Get number of babelnet Ids (synsets) for word\n",
        "  def get_synsets_num(self, word):\n",
        "    return len(self.get_synset_ids_word(word))\n",
        "\n",
        "  # Get word synsets Ids and positions of words\n",
        "  def get_word_bns(self, word):\n",
        "    babelnet_ids = []\n",
        "    bnid2pos = []\n",
        "    synsetids = [] \n",
        "\n",
        "    synsetids = self.get_synset_ids_word(word)\n",
        "    babelnet_ids = [synsetid[\"id\"] for synsetid in synsetids]\n",
        "    bnid2pos = {synset[\"id\"]:synset[\"pos\"] for synset in synsetids}\n",
        "\n",
        "    return babelnet_ids, bnid2pos\n",
        "\n",
        "  # Get specific synset information\n",
        "  def get_synset_info(self, synset_id):\n",
        "    if (__name__, synset_id) in self.cache:\n",
        "      synset = self.cache[(__name__, synset_id)]\n",
        "    else:\n",
        "      synset = self.bn.get_synset(id = synset_id, targetLang = self.targetLang)\n",
        "\n",
        "      # Add number of calls and add to cache dictionary\n",
        "      self.reqNum += 1\n",
        "      self.cache[(__name__, synset_id)] = synset\n",
        "\n",
        "    return synset\n",
        "\n",
        "  # Get simple lemmas of (senses) in synset, only verb-noun. Input - synsetId\n",
        "  def get_bn_lemmas_of_synset(self, synsetid, word): \n",
        "    senses_list = self.get_synset_info(synsetid)\n",
        "    \n",
        "    try:\n",
        "      senses_list = senses_list[\"senses\"]\n",
        "      word_capital = word[0].upper() + word[1:] # TODO Should it be removed?\n",
        "      lemmas = [sense[\"properties\"][\"simpleLemma\"] for sense in senses_list if (sense[\"properties\"][\"pos\"]=='VERB' or sense[\"properties\"][\"pos\"]=='NOUN') and\n",
        "                                                                              word_capital not in sense[\"properties\"][\"simpleLemma\"] and\n",
        "                                                                              sense[\"properties\"][\"language\"]==bn.targetLang]\n",
        "    except: \n",
        "      print(\"Synset ID: \",synsetid , \"has no senses for word '\", word, \"'.\")\n",
        "      lemmas = []\n",
        "    \n",
        "    return lemmas\n",
        "\n",
        "  # Get all words from all synsets of word (not unique)\n",
        "  def get_all_synsets_words(self, word):\n",
        "    babelnet_ids, bnid2pos = self.get_word_bns(word)\n",
        "    all_words = []\n",
        "    for id in babelnet_ids:\n",
        "      lemmas = self.get_bn_lemmas_of_synset(id, word)\n",
        "      all_words += lemmas\n",
        "    \n",
        "    return all_words\n",
        "\n",
        "  # ------------------------------------ EXTRA not in use for now ------------------------------------ #\n",
        "\n",
        "  # Get all hypernyms of synset\n",
        "  # Input - synsetId, lang\n",
        "  # Output - list\n",
        "  def get_hypernyms(self, synsetid, lang):\n",
        "    hypernyms = [(edge[\"target\"], edge[\"pointer\"][\"shortName\"]) for edge in self.bn.get_outgoing_edges(id=synsetid)\n",
        "                    if ((edge[\"language\"] == lang or edge[\"language\"] == \"MUL\") and edge[\"pointer\"][\"relationGroup\"] == \"HYPERNYM\")]\n",
        "    return hypernyms\n",
        "    \n",
        "  # Get all hyponyms of synset\n",
        "  # Input - synsetId, lang\n",
        "  # Output - list\n",
        "  def get_hyponyms(self, synsetid, lang):\n",
        "    hyponyms = [(edge[\"target\"], edge[\"pointer\"][\"shortName\"]) for edge in self.bn.get_outgoing_edges(id=synsetid)\n",
        "                    if ((edge[\"language\"] == lang or edge[\"language\"] == \"MUL\") and edge[\"pointer\"][\"relationGroup\"] == \"HYPONYM\")]\n",
        "    return hyponyms\n",
        "    \n",
        "  # Get all antonym of synset\n",
        "  # Input - synsetId, lang\n",
        "  # Output - list\n",
        "  def get_antonym(self, synsetid, lang):\n",
        "    antonym = [(edge[\"target\"], edge[\"pointer\"][\"shortName\"]) for edge in self.bn.get_outgoing_edges(id=synsetid)\n",
        "                    if ((edge[\"language\"] == lang or edge[\"language\"] == \"MUL\") and edge[\"pointer\"][\"relationGroup\"] == \"ANTONYM\")]\n",
        "    return antonym\n",
        "\n",
        "  # Get all other relations of synset\n",
        "  # Input - synsetId, lang\n",
        "  # Output - list\n",
        "  def get_other_relations(self, synsetid, lang):\n",
        "    others = [(edge[\"target\"], edge[\"pointer\"][\"shortName\"]) for edge in self.bn.get_outgoing_edges(id=synsetid)\n",
        "                    if ((edge[\"language\"] == lang or edge[\"language\"] == \"MUL\") and edge[\"pointer\"][\"relationGroup\"] == \"OTHER\")]\n",
        "    return others\n",
        "\n",
        "  # Get edges of synset\n",
        "  # Input - synsetId, lang\n",
        "  # Output - list\n",
        "  def get_edges_synset(self, synsetid, lang):\n",
        "    edges = [edge.target for edge in self.bn.get_outgoing_edges(id=synsetid)\n",
        "                    if edge.language == lang]\n",
        "    return edges\n"
      ],
      "metadata": {
        "id": "C04t0MJKuVrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransModel:\n",
        "\n",
        "  def __init__(self, src, trg):\n",
        "    self.model, self.tokenizer = self.__load_cache(src, trg)\n",
        "\n",
        "  def __load_cache(self, src, trg):\n",
        "    model_name = f\"Helsinki-NLP/opus-mt-{src}-{trg}\"\n",
        "    model_file_name = f\"/content/gdrive/MyDrive/Carinne-Thesis/Helsinki-NLP/opus-mt-{src}-{trg}.pkl\"\n",
        "    tokenizer_file_name = f\"/content/gdrive/MyDrive/Carinne-Thesis/Helsinki-NLP/opus-mt-{src}-{trg}_token.pkl\"\n",
        "    model = []\n",
        "    tokenizer = []\n",
        "\n",
        "    try:\n",
        "      with (open(model_file_name, \"rb\")) as openfile:\n",
        "        print(\"Found Model\")\n",
        "        while True:\n",
        "          try:\n",
        "            model.append(pickle.load(openfile))\n",
        "          except Exception as exp:\n",
        "            break\n",
        "    except Exception as exp:\n",
        "      print(exp)\n",
        "      print(f\"Model is downloading\")\n",
        "      model.append(MarianMTModel.from_pretrained(model_name, output_attentions = True))\n",
        "      \n",
        "      file = open(model_file_name, \"wb\")\n",
        "      pickle.dump(model[0], file)\n",
        "      file.close()\n",
        "\n",
        "    # try:\n",
        "    #   with (open(tokenizer_file_name, \"rb\")) as openfile:\n",
        "    #     print(\"Found Tokenizer\")\n",
        "    #     while True:\n",
        "    #       try:\n",
        "    #         tokenizer.append(pickle.load(openfile))\n",
        "    #       except Exception as exp:\n",
        "    #         break\n",
        "    # except Exception as exp:\n",
        "    print(f\"Tokenizer is downloading\")\n",
        "    tokenizer.append(MarianTokenizer.from_pretrained(model_name))\n",
        "\n",
        "    # file = open(tokenizer_file_name, \"wb\")\n",
        "    # pickle.dump(tokenizer[0], file)\n",
        "    # file.close()\n",
        "\n",
        "    return model[0], tokenizer[0]\n"
      ],
      "metadata": {
        "id": "VOY9JvJ1uWFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set languages of research\n",
        "src = \"es\" #bn.searchLang\n",
        "trg = \"he\" #bn.targetLang\n",
        "\n",
        "marianFront = TransModel(src, trg)\n",
        "marianBack = TransModel(trg, src)\n",
        "similarity = SentenceTransformer('sentence-transformers/multi-qa-MiniLM-L6-cos-v1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBwfIzI0F0WR",
        "outputId": "71948f92-98f1-41ee-fa46-81ed77f493ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found Model\n",
            "Tokenizer is downloading\n",
            "Found Model\n",
            "Tokenizer is downloading\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download configuration from huggingface.co and cache.\n",
        "gcFront = GenerationConfig.from_pretrained(\"Helsinki-NLP/opus-mt-es-he\")\n",
        "gcBack = GenerationConfig.from_pretrained(\"Helsinki-NLP/opus-mt-he-es\")\n",
        "\n",
        "texts = [['Gloria a Ucrania, un saludo patriótico.',\n",
        "          'Pero con el tiempo y la exposición, Ginsburg dijo que ha desarrollado su tolerancia a la hierba.',\n",
        "          'La desventaja: en muchas áreas, sigue siendo un mercado de vendedores.',\n",
        "          'Pie de foto, Homenaje a Flora Tristán en Francia.',\n",
        "          'De esos casos, 27 la boquilla terminó en la boca de los niños.',\n",
        "          'Tu inscripción ha sido exitosa.',\n",
        "          'Guardar tus artículos favoritos.',\n",
        "          'Powerball histórico: un californiano ganó el premio más grande las loterías en EE.UU.']]\n",
        "i = 0\n",
        "for doc in texts:\n",
        "  print(i)\n",
        "  if doc is not None:\n",
        "    for t in doc:\n",
        "\n",
        "      print(\"Source: \", t)\n",
        "\n",
        "      # Forward translate\n",
        "      batch = marianFront.tokenizer.encode(t, return_tensors = \"pt\")\n",
        "      generated_ids = marianFront.model.generate(batch, output_attentions=True, generation_config= gcFront, return_dict_in_generate=True, max_new_tokens = 512)\n",
        "      heb_sentence = marianFront.tokenizer.batch_decode(generated_ids.sequences, skip_special_tokens = True)[0]\n",
        "\n",
        "      print(\"Hebrew translation: \", heb_sentence)\n",
        "\n",
        "      # Backward Translate\n",
        "      batch = marianBack.tokenizer.encode(heb_sentence, return_tensors = \"pt\")\n",
        "      generated_ids = marianBack.model.generate(batch, output_attentions=True, generation_config=gcBack, return_dict_in_generate=True, max_new_tokens = 512)\n",
        "      spa_sentence = marianBack.tokenizer.batch_decode(generated_ids.sequences, skip_special_tokens = True)[0]\n",
        "\n",
        "      print(\"Spanish translation: \", spa_sentence)\n",
        "\n",
        "      # How far is the result from the source using similarity model?\n",
        "      embeddings1 = similarity.encode(t, convert_to_tensor=True)\n",
        "      embeddings2 = similarity.encode(spa_sentence, convert_to_tensor=True)\n",
        "\n",
        "      # Compute cosine-similarities\n",
        "      score = util.cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "      print(\"Score: \", score.item(), \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lV0uCXIGI7u",
        "outputId": "3df80f72-696a-47eb-9874-81ba685a0a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "Source:  Gloria a Ucrania, un saludo patriótico.\n",
            "Hebrew translation:  גלוריה לאוקראינה, ברכה פטריוטית.\n",
            "Spanish translation:  Gloria a Ucrania, una bendición patriótica.\n",
            "Score:  0.9529401659965515 \n",
            "\n",
            "Source:  Pero con el tiempo y la exposición, Ginsburg dijo que ha desarrollado su tolerancia a la hierba.\n",
            "Hebrew translation:  אבל עם הזמן והחשיפה, גינסבורג אמרה שהיא פיתחה סובלנות לגראס.\n",
            "Spanish translation:  Pero con el tiempo y la exposición, Ginsburg dijo que había desarrollado tolerancia a la hierba.\n",
            "Score:  0.9928410649299622 \n",
            "\n",
            "Source:  La desventaja: en muchas áreas, sigue siendo un mercado de vendedores.\n",
            "Hebrew translation:  החיסרון: באזורים רבים, זה עדיין שוק של אנשי מכירות.\n",
            "Spanish translation:  La desventaja es que en muchas zonas, sigue siendo un mercado de vendedores.\n",
            "Score:  0.8015527129173279 \n",
            "\n",
            "Source:  Pie de foto, Homenaje a Flora Tristán en Francia.\n",
            "Hebrew translation:  תמונת כף הרגל, הוקרה לפלורה טריסטן בצרפת.\n",
            "Spanish translation:  La foto de los pies, un homenaje a Flora Tristán en Francia.\n",
            "Score:  0.9387475848197937 \n",
            "\n",
            "Source:  De esos casos, 27 la boquilla terminó en la boca de los niños.\n",
            "Hebrew translation:  במקרה כזה, 27 זרבוביות נספו בפיהם של הילדים.\n",
            "Spanish translation:  En ese caso, 27 boquillas murieron en la boca de los niños.\n",
            "Score:  0.8973571062088013 \n",
            "\n",
            "Source:  Tu inscripción ha sido exitosa.\n",
            "Hebrew translation:  ההקדשה שלך הצליחה.\n",
            "Spanish translation:  Tu dedicación ha funcionado.\n",
            "Score:  0.6094968318939209 \n",
            "\n",
            "Source:  Guardar tus artículos favoritos.\n",
            "Hebrew translation:  שמור את הפריטים האהובים עליך.\n",
            "Spanish translation:  Guarda tus artículos favoritos.\n",
            "Score:  0.9648479223251343 \n",
            "\n",
            "Source:  Powerball histórico: un californiano ganó el premio más grande las loterías en EE.UU.\n",
            "Hebrew translation:  פאוורבול היסטורי: קליפורניה זכתה בלוטו הגדול ביותר בארה\"ב.\n",
            "Spanish translation:  \"California ganó la lotería más grande de EE.UU.\"\n",
            "Score:  0.6628783941268921 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bn = BabelNet('ES', 'ES')"
      ],
      "metadata": {
        "id": "6hbLJY-CHOf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# words = ['inscripción', 'dedicación']\n",
        "# words = ['áreas','zonas']\n",
        "words = ['saludo','bendición']"
      ],
      "metadata": {
        "id": "9wObnkClBIMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "words = [stemmer.stem(w) for w in words]"
      ],
      "metadata": {
        "id": "iyTu5aByirDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def new_layer(nodes, graph):\n",
        "  for id in nodes:\n",
        "    antonyms = [h for h in bn.get_antonym(synsetid=id, lang = src) if h != id]\n",
        "    for anto in antonyms:\n",
        "      graph.add_nodes_from([(anto[0], {'lang': bn.targetLang, 'color':'#AAC8Ff2'})], data = True)\n",
        "      graph.add_edge(id, anto[0], color='blue', key='anto', connectionstyle='arc3, rad = 0', weight = 3, rad=0.1, desc=anto[1])\n",
        "\n",
        "    hyponyms = [h for h in bn.get_hyponyms(synsetid=id, lang = src) if h != id]\n",
        "    for hypo in hyponyms:\n",
        "      graph.add_nodes_from([(hypo[0], {'lang': bn.targetLang, 'color':'#BEF2AA'})], data = True)\n",
        "      graph.add_edge(id, hypo[0], color='green', key='hypo', connectionstyle='arc3, rad = 0', weight = 2, rad=0.1, desc=hypo[1])\n",
        "\n",
        "    hypernyms = [h for h in bn.get_hypernyms(synsetid=id, lang = src) if h != id]\n",
        "    for hyper in hypernyms:\n",
        "      graph.add_nodes_from([(hyper[0], {'lang': bn.targetLang, 'color':'#F1F2AA'})], data = True)\n",
        "      graph.add_edge(hyper[0], id, color='yellow', key='hyper', connectionstyle='arc3, rad = 0', weight = 1, rad=0.1, desc=hyper[1])\n",
        "\n",
        "    others = [h for h in bn.get_other_relations(synsetid=id, lang = src) if h != id]\n",
        "    for o in others:\n",
        "      graph.add_nodes_from([(o[0], {'lang': bn.targetLang, 'color':'#FAC2BE'})], data = True)\n",
        "      graph.add_edge(id, o[0], color='red', key='other', connectionstyle='arc3, rad = 0', weight = 0.5, rad=0.1, desc=o[1])"
      ],
      "metadata": {
        "id": "UyjrimyHXYp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define graph of connections between words and their sysnsets\n",
        "def graph_en_to_es(words) -> nx.MultiGraph:\n",
        "  graph = nx.MultiGraph()\n",
        "\n",
        "  # Get translations for each word in list (from babel) and add to graph\n",
        "  # for w in random.sample(words, 10):\n",
        "  for w in words:\n",
        "    w = w.lower()\n",
        "    print(w)\n",
        "    graph.add_nodes_from([(w, {'lang': bn.searchLang, 'color':'#D3D3D3'})])\n",
        "    synsetIds = set([row.get('id') for row in bn.bn.get_synset_ids(lemma = w, searchLang = 'ES', targetLang = 'ES')])\n",
        "\n",
        "    for id in synsetIds:\n",
        "      graph.add_nodes_from([(id, {'lang': bn.targetLang, 'color':'#CBC3E3'})], data = True)\n",
        "      graph.add_edge(w, id, color='gray', key='babel', connectionstyle='arc3, rad = 0', weight = 4, rad=0.1, desc=\"root\")\n",
        "\n",
        "      new_layer([id], graph)\n",
        "        \n",
        "  return graph"
      ],
      "metadata": {
        "id": "DhEhGdF3bj0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_graph = graph_en_to_es(words)\n",
        "main_graph_depth = 2\n",
        "\n",
        "from networkx.drawing.layout import bipartite_layout\n",
        "from networkx import bipartite\n",
        "plt.rcParams[\"figure.figsize\"] = (20,40)\n",
        "\n",
        "# Print graph\n",
        "def print_graph(G, save, file):\n",
        "  pos = nx.bipartite_layout(G, words, scale=2)\n",
        "  edges = G.edges(data=True)\n",
        "  colors_edge = nx.get_edge_attributes(G,'color').values()\n",
        "  colors_node = nx.get_node_attributes(G,'color').values()\n",
        "  edge_styles = nx.get_edge_attributes(G,'connectionstyle').values()\n",
        "  edge_weights = list(nx.get_edge_attributes(G,'weight').values())\n",
        "\n",
        "  nx.draw(G, node_color = colors_node, edge_color = colors_edge, node_size=200, width = edge_weights, with_labels = True)\n",
        "  \n",
        "  if save:\n",
        "    plt.savefig(file, format=\"PNG\")\n",
        "  plt.show()\n",
        "\n",
        "print_graph(main_graph, True, \"main.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "t1Sw7a8kbkxp",
        "outputId": "1b0d04c8-cd20-4e7c-a059-492ecb7c9a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saludo\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d1e4ed47f95a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_en_to_es\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmain_graph_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbipartite_layout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbipartite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-05e816974f4b>\u001b[0m in \u001b[0;36mgraph_en_to_es\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'lang'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchLang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'color'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'#D3D3D3'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msynsetIds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_synset_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearchLang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ES'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetLang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ES'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynsetIds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-05e816974f4b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_nodes_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'lang'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchLang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'color'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'#D3D3D3'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msynsetIds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_synset_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearchLang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ES'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetLang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ES'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynsetIds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# while not nx.has_path(main_graph, words[0], words[1]):\n",
        "#   # Get leaves of graph, last layer\n",
        "#   nodes = set(node for node, distance in nx.shortest_path_length(main_graph, words[0]).items() if distance == main_graph_depth)\n",
        "#   new_layer(nodes = nodes, graph = main_graph)\n",
        "#   main_graph_depth += 1\n",
        "#   pass"
      ],
      "metadata": {
        "id": "_c2YelddgD8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print_graph(main_graph, True, \"main.png\")"
      ],
      "metadata": {
        "id": "jUGJAuYohk9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nx.shortest_path_length(main_graph, words[0]).items()"
      ],
      "metadata": {
        "id": "fH7r1BzFfPIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of same arc type from node \n",
        "def num_same_arcs(node, arc_type_r):\n",
        "  return sum(1 if next(iter(main_graph.get_edge_data(e[0], e[1]).keys())) == arc_type_r else 0 for e in iter(main_graph.edges(node)))  \n",
        "\n",
        "# Node depth\n",
        "def calc_node_depth(node):\n",
        "  return min(nx.shortest_path_length(main_graph, source = node, target = r) for r in words)\n",
        "\n",
        "# Edge weight\n",
        "def calc_edge_weight(node_start, node_end):\n",
        "  # Get edge type\n",
        "  edge_data = main_graph.get_edge_data(node_start, node_end)\n",
        "  edge_typeR = next(iter(edge_data.keys()))\n",
        "\n",
        "  if edge_typeR == \"anto\":\n",
        "    weight = 2.5\n",
        "  elif edge_typeR == \"babel\":\n",
        "    weight = 0\n",
        "  else:\n",
        "    maxR = 2\n",
        "    minR = 1\n",
        "\n",
        "    # Count number of same arcs for start node\n",
        "    num_arc_start = num_same_arcs(node_start, edge_typeR)\n",
        "    num_arc_end = num_same_arcs(node_end, edge_typeR)\n",
        "\n",
        "    # Get number of same arcs \n",
        "    weight_start = maxR - ((maxR - minR)/num_arc_start)\n",
        "    weight_end = maxR - ((maxR - minR)/num_arc_end)\n",
        "\n",
        "    weight = (weight_start + weight_end) /2.0\n",
        "\n",
        "  return weight\n",
        "\n",
        "# Path weight\n",
        "def distance(node_start, node_end):\n",
        "  path = nx.shortest_path(main_graph,node_start,node_end)\n",
        "  total_weight = 0\n",
        "\n",
        "  for i in range(len(path)-1):\n",
        "    total_weight += calc_edge_weight(path[i], path[i+1])\n",
        "\n",
        "  return total_weight"
      ],
      "metadata": {
        "id": "7uegqt4QgMIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distance(words[0],words[1])"
      ],
      "metadata": {
        "id": "OxTXYfkFPx7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[n for n,d in G.in_degree() if d==0]"
      ],
      "metadata": {
        "id": "v6ldg10MI6Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get shortest path and calculate weight"
      ],
      "metadata": {
        "id": "iwYNa_SXiP2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# graph_add_relation_edges(graph) 00085163v  00088223v\n",
        "path = nx.shortest_path(main_graph,'pero','gato')\n",
        "\n",
        "for i in range(len(path)-1):\n",
        "  print(path[i], path[i+1])"
      ],
      "metadata": {
        "id": "ZYbZzqT6n9WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nx.shortest_path_length(main_graph, source='bn:00076248n', target='bn:01686524n')"
      ],
      "metadata": {
        "id": "i7lnwnXgHBvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path"
      ],
      "metadata": {
        "id": "aM9Bpjs0Pcbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_graph.get_edge_data('bn:00041739n', 'bn:00041739n')"
      ],
      "metadata": {
        "id": "t8SaiiVuEn8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bn.bn.get_synset_ids(lemma = 'saludo', searchLang = 'ES', targetLang = 'ES')"
      ],
      "metadata": {
        "id": "euCKEiUd_qEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bn.get_other_relations(synsetid='bn:00041739n', lang='ES')[0][1]"
      ],
      "metadata": {
        "id": "dqseV6VxJ37G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bn.bn.get_outgoing_edges(id='bn:00041739n')"
      ],
      "metadata": {
        "id": "cN62cjSGB70i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bn.bn.get_synset(id = \"bn:00067050n\", targetLang = \"ES\")"
      ],
      "metadata": {
        "id": "UWiJL3kPfC-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for edge in bn.bn.get_outgoing_edges(id=\"bn:00088223v\"):\n",
        "  print(edge)\n",
        "  "
      ],
      "metadata": {
        "id": "6DVz303EIIYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in bn.bn.get_outgoing_edges(id='bn:00041739n'):\n",
        "  print(row[\"pointer\"][\"relationGroup\"])"
      ],
      "metadata": {
        "id": "T5BIAd0NCKoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bn.bn.get_synset(id = 'bn:00041739n', targetLang = 'ES')"
      ],
      "metadata": {
        "id": "ruiDRufPBKpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YNiqsZUZB1lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in bn.bn.get_synset_ids(lemma = 'salute', searchLang = 'ES', targetLang = 'ES'):\n",
        "  print(row.get('id'))"
      ],
      "metadata": {
        "id": "9Epg1kXKc1vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "42z2t2BUEmZ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}